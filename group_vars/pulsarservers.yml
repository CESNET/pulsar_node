# Put your Galaxy server's fully qualified domain name (FQDN) (or the FQDN of the RabbitMQ server) above.
rabbitmq_hostname: "galaxy-re.grid.cesnet.cz" # DEMON: Important to have proper FQDN of connected Galaxy server (or at least RabbitMQ server)
rabbitmq_vhost: "/pulsar" # DEMON: must be the same rabbitmq vhost set for communication between Galaxy and Pulsar set on Galaxy side
rabbitmq_user: "pulsar" # DEMON: must be user running connected Galaxy server
#rabbitmq_user: "{{ galaxy_user.name }}" # DEMON: must be user running connected Galaxy server
rabbitmq_port: 5671 # DEMON: must be port where to contact RabbitMQ server

pulsar_root: /opt/pulsar
pulsar_package_version: "0.15.3"
pulsar_data_dir: "/auto/{{ galaxy_user.nfs_home }}/home/{{ galaxy_user.name }}/{{ pulsar_prefix }}"
pulsar_staging_dir: "{{ pulsar_data_dir }}/files/staging"
pulsar_persistence_dir: "{{ pulsar_root }}/files/persistent"
pulsar_dependencies_dir: "{{ pulsar_data_dir }}/deps"
pulsar_separate_privileges: true

miniconda_prefix: "/auto/{{ galaxy_user.nfs_home }}/home/{{ galaxy_user.name }}/conda"
miniconda_version: 4.12.0
miniconda_base_env_packages: ['mamba']

conda_prefix: "{{ miniconda_prefix }}"
conda_exec: "{{ conda_prefix }}/bin/mamba"
conda_channels:
  - "conda-forge"
  - "bioconda"
  - "iuc"
  - "petrnovak"
  - "r"
  - "anaconda"
  - "defaults"

pulsar_pip_install: true
pulsar_pycurl_ssl_library: openssl
pulsar_systemd: true
pulsar_systemd_enabled: true
pulsar_systemd_runner: webless
pulsar_systemd_service_name: pulsar
pulsar_systemd_state: started
pulsar_systemd_memory_limit: 6
pulsar_systemd_environment:
  - DRMAA_LIBRARY_PATH="/home/{{ galaxy_user.name }}/pbs-drmaa-{{ pbs_drmaa_version }}/pbs_drmaa/.libs/libdrmaa.so"
pulsar_systemd_nofile_limit: 100000

pulsar_create_user: false
pulsar_user:
  name: "{{ galaxy_user.name }}"
  shell: /bin/bash
#pulsar_group: "{{ pulsar_user.name }}"

pulsar_optional_dependencies:
  - pyOpenSSL
  # For remote transfers initiated on the Pulsar end rather than the Galaxy end
  - pycurl
  # drmaa required if connecting to an external DRM using it.
  - drmaa
  # kombu needed if using a message queue
  - 'kombu==5.3.2'
  - 'amqp==5.1.1'
  # DEMON: it used to be set for version 5.0.2 both kombu and amqp, but it seems that newer versions work too
  # amqp 5.0.3 changes behaviour in an unexpected way, pin for now.
  #- 'amqp==5.0.2'
  #- 'kombu==5.0.2'
  # psutil and pylockfile are optional dependencies but can make Pulsar
  # more robust in small ways.
  - psutil
  - pylockfile

pulsar_yaml_config:
  staging_directory: "{{ pulsar_staging_dir }}"
  persistence_directory: "{{ pulsar_persistence_dir }}"
  tool_dependency_dir: "{{ pulsar_dependencies_dir }}"
  # The following are the settings for the pulsar server to contact the message queue with related timeouts etc.
  message_queue_url: "pyamqp://{{ rabbitmq_user }}:{{ rabbitmq_password }}@{{ rabbitmq_hostname }}:{{ rabbitmq_port }}/{{ rabbitmq_vhost }}?ssl=1"
  min_polling_interval: 0.5
  amqp_publish_retry: True
  amqp_publish_retry_max_retries: 5
  amqp_publish_retry_interval_start: 10
  amqp_publish_retry_interval_step: 10
  amqp_publish_retry_interval_max: 60
  # We also need to create the dependency resolvers configuration so pulsar knows how to find and install dependencies
  # for the tools we ask it to run. The simplest method which covers 99% of the use cases is to use conda auto installs
  # similar to how Galaxy works.
  dependency_resolution:
    resolvers:
      - type: conda
        #auto_init: true
        auto_init: false
        auto_install: true
        ensure_channels: "{{ conda_channels|join(',') }}"
        exec: "{{ conda_exec }}"
        prefix: "{{ conda_prefix }}"
#  container_resolvers: 
#    - type: explicit_singularity
#      cache_directory: "/storage/brno11-elixir/home/galaxyelixir/pulsar-re/singularity"
#    - type: cached_mulled_singularity
#      cache_directory: "/storage/brno11-elixir/home/galaxyelixir/pulsar-re/singularity"
#    - type: mulled_singularity
#      cache_directory: "/storage/brno11-elixir/home/galaxyelixir/pulsar-re/singularity"
  managers:
    _default_:
      type: queued_cli
      job_plugin: Torque
      job_Resource_List: "select=1:ncpus=2:mem=8gb:pbs_server=elixir-pbs.elixir-czech.cz:scratch_local=50gb,walltime=24:00:00"
      job_destination: "elixirre@elixir-pbs.elixir-czech.cz"
      job_Wide_setup: "umask=022"
    long_and_big:
      type: queued_cli
      job_plugin: Torque
      job_Resource_List: "select=1:ncpus=16:mem=64gb:pbs_server=elixir-pbs.elixir-czech.cz:scratch_local=150gb,walltime=48:00:00"
      job_destination: "elixirre@elixir-pbs.elixir-czech.cz"
    gpu:
      type: queued_cli
      job_plugin: Torque
      job_Resource_List: "select=1:ncpus=12:mem=120gb:pbs_server=meta-pbs.metacentrum.cz:scratch_local=100gb:ngpus=1:gpu_mem=8gb,walltime=12:00:00"
      job_destination: "gpu@meta-pbs.metacentrum.cz"
    test:
      type: queued_drmaa
      preprocess_action_max_retries: 10
      preprocess_action_interval_start: 2
      preprocess_action_interval_step: 2
      preprocess_action_interval_max: 30
      postprocess_action_max_retries: 10
      postprocess_action_interval_start: 2
      postprocess_action_interval_step: 2
      postprocess_action_interval_max: 30
      ### Better to let Galaxy decides about this in job_conf
      #native_specification: "-l select=1:ncpus=2:mem=8gb:scratch_local=50gb -l walltime=23:59:59 -q elixirre@elixir-pbs.elixir-czech.cz"

# Pulsar should use the same job metrics plugins as Galaxy. This will automatically set `job_metrics_config_file` in
# `pulsar_yaml_config` and create `{{ pulsar_config_dir }}/job_metrics_conf.yml`.
#pulsar_job_metrics_plugins: "{{ galaxy_job_metrics_plugins }}"
